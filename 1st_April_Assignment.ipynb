{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc989d-1583-407d-bbc4-287d463e72b1",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273e96b-45b9-4337-80ba-f3d5e6de5b0d",
   "metadata": {},
   "source": [
    "Linear Regression:                                                                                  \n",
    "\n",
    "Linear Regression is one of the most simple Machine learning algorithm that comes under Supervised Learning technique and used for solving regression problems.                                                                      \n",
    "It is used for predicting the continuous dependent variable with the help of independent variables.                       \n",
    "The goal of the Linear regression is to find the best fit line that can accurately predict the output for the continuous dependent variable.                                                                                    \n",
    "If single independent variable is used for prediction then it is called Simple Linear Regression and if there are more than two independent variables then such regression is called as Multiple Linear Regression.                          \n",
    "By finding the best fit line, algorithm establish the relationship between dependent variable and independent variable. And the relationship should be of linear nature.                                                               \n",
    "The output for Linear regression should only be the continuous values such as price, age, salary, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122b016-1f5b-430a-96c9-e5a0e160baa7",
   "metadata": {},
   "source": [
    "Logistic Regression:                                                                                                                        \n",
    "\n",
    "Logistic regression is one of the most popular Machine learning algorithm that comes under Supervised Learning techniques.\n",
    "It can be used for Classification as well as for Regression problems, but mainly used for Classification problems.      \n",
    "Logistic regression is used to predict the categorical dependent variable with the help of independent variables.         \n",
    "The output of Logistic Regression problem can be only between the 0 and 1.                                           \n",
    "Logistic regression can be used where the probabilities between two classes is required. Such as whether it will rain today or not, either 0 or 1, true or false etc.                                                              \n",
    "Logistic regression is based on the concept of Maximum Likelihood estimation. According to this estimation, the observed data should be most probable.                                                                                   \n",
    "In logistic regression, we pass the weighted sum of inputs through an activation function that can map values in between 0 and 1. Such activation function is known as sigmoid function and the curve obtained is called as sigmoid curve or S-curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02882c0-a584-4895-9f56-0c804d216be1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca1b98-684f-4030-97a3-0a563e264d46",
   "metadata": {},
   "source": [
    "Logistic Regression is used when the dependent variable(target) is categorical. For example, To predict whether an email is spam (1) or (0) Whether the tumor is malignant (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98d02a-72f9-4a15-83c0-c6ee49508cca",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3574f67-eab6-4a33-b222-6aa82f41a8a8",
   "metadata": {},
   "source": [
    " For Logistic Regression,                              \n",
    "\n",
    "h_{\\Theta}(x) = g(\\Theta^{T}x)                                            \n",
    "\n",
    "It will result in a non-convex cost function as shown above. So, for Logistic Regression the cost function we use is also known as the cross entropy or the log loss.                                                    \n",
    "\n",
    "Cost(h_{\\Theta}(x),y) = \\left\\{\\begin{matrix} -log(h_{\\Theta}(x)) & if&y=1\\\\ -log(1-h_{\\Theta}(x))& if& y = 0 \\end{matrix}\\right.                                                                \n",
    "\n",
    "Case 1: If y = 1, that is the true label of the class is 1. Cost = 0 if the predicted value of the label is 1 as well. But as hθ(x) deviates from 1 and approaches 0 cost function increases exponentially and tends to infinity which can be appreciated from the below graph as well.                                                                     \n",
    "\n",
    "\n",
    "Case 2: If y = 0, that is the true label of the class is 0. Cost = 0 if the predicted value of the label is 0 as well. But as hθ(x) deviates from 0 and approaches 1 cost function increases exponentially and tends to infinity which can be appreciated from the below graph as well.                                                      \n",
    "\n",
    "\n",
    "With the modification of the cost function, we have achieved a loss function that penalizes the model weights more and more as the predicted value of the label deviates more and more from the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842cddf-c2db-4ed9-b358-ee4c4fc1f769",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb0c79-059c-493c-ab5b-b86e0d7e5568",
   "metadata": {},
   "source": [
    "Regularization is a technique used to avoid overfitting in machine learning models. It does this by adding a penalty term to the objective function (also called the loss function or error function) that the model is trying to minimize.              \n",
    "\n",
    "The objective function measures the error or difference between the predicted output of the model and the true output. In logistic regression, the objective function is typically the cross-entropy loss, which measures the difference between the predicted probability of the positive class and the true label (1 or 0).                                               \n",
    "\n",
    "By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely. The penalty term is a hyperparameter that controls the strength of the regularization. A higher value for the penalty term leads to stronger regularization and a simpler model, while a lower value allows the model to be more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4875f-65db-47d2-a42c-2952dcbaa078",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19efc19-ed69-4899-a582-19df0bde5255",
   "metadata": {},
   "source": [
    "The RoC curve is a graphical representation of the performance of a binary classifier. It plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. In simpler terms, the RoC curve is a way to visualize how well a classifier is able to distinguish between positive and negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541dcd7-e4c5-4676-b6cf-058c40f01663",
   "metadata": {},
   "source": [
    "To use the RoC curve with logistic regression, we need to first train our model on a labeled dataset. Once we have trained our model, we can use it to predict the class probabilities of new instances. We can then use these predicted probabilities to generate an RoC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f32ac9-fe48-4c93-a2b8-26d78c5e3263",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5f397-8cad-480d-9785-137094ee0a03",
   "metadata": {},
   "source": [
    "The choice of feature selection technique depends on the type and amount of data available, as well as the modeling approach. It’s important to experiment with different methods to find the best approach for a given problem.         \n",
    "\n",
    "Filter methods: These methods rank the features based on statistical measures such as correlation, mutual information, or chi-squared tests. Features with the highest scores are selected for the model.                                       \n",
    "Wrapper methods: These methods involve training and evaluating the model with different subsets of features, using a search algorithm to find the optimal set of features that maximizes model performance.                            \n",
    "Embedded methods: These methods incorporate feature selection into the model training process, selecting the most relevant features during the training of the model.                                                \n",
    "Principal Component Analysis (PCA): This method transforms the data into a lower-dimensional space by identifying linear combinations of features that capture the most significant variability in the data.                             \n",
    "Recursive Feature Elimination (RFE): This method iteratively removes the least important features from the model until the desired number of features is reached.                             \n",
    "Lasso Regression: This method performs regularization by adding a penalty term to the model’s loss function, which encourages the model to select a sparse set of features.                           \n",
    "Genetic Algorithms: These methods use an evolutionary search algorithm to find the optimal set of features that maximizes model performance.                                                     \n",
    "Univariate Feature Selection: This method selects the features that have the strongest relationship with the target variable, based on statistical tests such as ANOVA or t-tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c92a81-b581-4561-9dfa-21a17c304522",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e75ace-4236-4845-907c-54c3e839ecb6",
   "metadata": {},
   "source": [
    "1. Oversampling                                                                         \n",
    "\n",
    "In this approach, we synthesize new examples from the minority class.                                      \n",
    "\n",
    "There are several methods available to oversample a dataset used in a typical classification problem. But the most common data augmentation technique is known as Synthetic Minority Oversampling Technique or SMOTE for short. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678445ce-2aa4-4bc8-86ef-92e7ac166588",
   "metadata": {},
   "source": [
    "2. Undersampling                                                           \n",
    "In this approach, we reduce the number of samples from the majority class to match the number of samples in the minority class. \n",
    "\n",
    "\n",
    "How to handle imbalanced data – algorithm approach                                            \n",
    "\n",
    "This approach concentrates on modifying existing models to alleviate their bias towards the majority groups. This requires good insight into the modified learning algorithm and precise identification of reasons for its failure in learning the representations of skewed distributions.                                               \n",
    "\n",
    "The most popular techniques are cost-sensitive approaches (weighted learners). Here, the given model is modified to incorporate varying penalties for each considered group of examples. In other words, we use Focal loss where we assign a higher weight to the minority class in our cost function which will penalize the model for misclassifying the minority class while at the same time reducing the weight of the majority class, causing the model to pay more attention to the underrepresented class. Thus, boosting its importance during the learning process.                                   \n",
    "\n",
    "Another interesting algorithm-level solution is to apply one-class learning or one-class classification(OCC for short) that focuses on the target group, creating a data description. This way we eliminate bias towards any group, as we concentrate only on a single set of objects.                                                                 \n",
    "\n",
    "OCC can be useful in imbalanced classification problems because it provides techniques for outlier and anomaly detection. It does this by fitting the model on the majority class data (also known as positive examples) and predicting whether new data belong to the majority class or belong to the minority class(also known as negative examples) meaning it’s an outlier/anomaly.                                                                                       \n",
    "\n",
    "OCC problems usually are practical classification tasks where majority class data is easily available but minority class is hard, expensive, and even impossible to gather, i.e. work of an engine, fraudulent transactions, intrusion detection for the computer system, and so on.                                                            \n",
    "\n",
    "How to deal with imbalanced data – hybrid approach                                                                         \n",
    "Hybridization is an approach that exploits the strengths of individual components. When it comes to dealing with imbalanced classification data, some works proposed hybridization of sampling and cost-sensitive learning. In other words, combining both data and algorithm level approaches. This idea of two-stage training that merges data-level solutions with algorithm-level solutions (i.e. classifier ensemble), resulting in robust and efficient learners is highly popular.                                                                                                                                                 \n",
    "It works by applying a data-level approach first. As you remember the data level approach works by modifying the training set to balance the class distribution between the majority class and the minority by using either oversampling or undersampling.                                            \n",
    "\n",
    "Then the pre-processed data with balanced class distribution is used to train a classifier ensemble, in other words, a collection of multiple classifiers from which a new classifier is derived which performs better than any constituent classifier. Thus, creating a robust and efficient learner that inherits the strong points of both data and algorithm level approaches while reducing their weaknesses at the same time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7625e-f51f-4189-8cde-eb85c2b63c17",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cfa62-c1db-4628-85b5-81618aef3f9e",
   "metadata": {},
   "source": [
    "The challenges will depend primarily on the scale of the problem being solved. In short though, the main challenge will be to implement an optimizer correctly and in a scalable way.                                                                \n",
    "\n",
    "There are good implementations of logistic regression (cf R package glmnet and hadoop based implementation in Spark) that can handle fairly large datasets, especially sparse ones, given today's commodity hardware. I suggest leveraging these implementations as much as possible.                                                                         \n",
    "\n",
    "If for whatever reason you have to implement a logistic solver yourself, based on my past experience implementing regression model solvers that run on billions of observations and millions of variables, I can say that determining a suitable optimization method and implementing it correctly will require a substantial amount of effort (but is a ton of fun and a great learning experience).                                                            \n",
    "\n",
    "A convenient property of logistic regression is that its objective function is convex, so gradient based optimization will yield globally optimal solution. There are a number of techniques that can be applied for solving the correpsonding optimization problem. Broadly these can be divided into first and second order methods. Examples of first order methods include (stochastic) gradient descent and its accelerated variant. Second order methods such as Newton-Raphson method involve the Hessian, which in practice is often approximated by a diagonal matrix. Second order methods can converge faster. First order methods are fairly simple to implement. Note that if you'd like the solver to support L1 regularization, it will make the problem nonsmooth and will require additional handling. L2 regularization is straightforward to incorporate in the optimization since it remains smooth.                                                                    \n",
    "\n",
    "Finally, if the input data is sparse and the solver needs to support regularization, the solver can be made more efficient by honoring sparsity of the data without having to update coefficients for all parameters on each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadd8ce-e9ed-4e8d-a5e0-90e41b9f78d1",
   "metadata": {},
   "source": [
    "To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
