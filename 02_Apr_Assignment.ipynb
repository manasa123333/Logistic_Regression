{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e073acf-b85c-4843-92bc-40b68e3a2286",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177cb9d-5f23-4708-bf72-14d99ac0faf6",
   "metadata": {},
   "source": [
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It's essentially a cross-validation technique. The model as well as the parameters must be entered. After extracting the best parameter values, predictions are made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe724475-cece-4c23-a2c9-e1c48744c6ad",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a91ef8-3eb7-4f0b-99b7-556b619efb90",
   "metadata": {},
   "source": [
    "In grid search cv we use very possible combinations between parameters and then find the best parameters using accuracy.         \n",
    "In Randomize search only certain amount of combinations are considered and then find the best parameters using accuracy.             \n",
    "We use grid search cv when we have lesser amount of data. But if we have large amount of data we use randomize search cv because of time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a948a76-3c67-4411-b6e0-dad130e64921",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d800a-ad9e-461e-9b2c-a7eebc865484",
   "metadata": {},
   "source": [
    "If a test datapoint is already present in train dataset then this problem is known as data leakage. . It causes high performance while training set, but perform poorly in deployment or production.                                                                                                                                                                             \n",
    "\n",
    "Here are common examples: Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications. Data exposed at rest — Can occur due to misconfigured cloud storage, insecure databases, or unattended or lost devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f63752-fdcb-460f-8b3c-576b49d43d3a",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf6214-94ae-4190-8d80-d9862fab2755",
   "metadata": {},
   "source": [
    "6 Ways to Help Prevent Data Leakage:                                                  \n",
    "1. Understanding the Dataset                                                \n",
    "2. Cleaning Dataset for Duplicates                                                          \n",
    "3. Selecting Features with Regard to Target Variable Correlation and Temporal Ordering                                 \n",
    "4. Splitting Dataset into Train, Validation, and Test Groups                                    \n",
    "5. Normalizing After Splitting, BUT Before Cross Validation                                                        \n",
    "6. Assessing Model Performance with a Healthy Skepticism                                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ba526-5d3c-4df4-ad64-a9a139de7999",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26071997-5d14-49d8-a2db-f417d1a09042",
   "metadata": {},
   "source": [
    "A confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives. This matrix aids in analyzing model performance, identifying mis-classifications, and improving predictive accuracy                                      \n",
    "The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fa57a-757c-4b08-8ef4-6bb16501eb5c",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cadc1-402d-41ad-98fe-03524bd0977e",
   "metadata": {},
   "source": [
    "Precision tells us how many of the correctly predicted cases actually turned out to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbde3ce-db88-46b6-a0f7-24715674dec0",
   "metadata": {},
   "source": [
    "Recall tells us how many of the actual positive cases we were able to predict correctly with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce267f-18f2-41d8-8656-d0e43387da21",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9547e9-6063-45ec-bd71-4f31a89d6bb0",
   "metadata": {},
   "source": [
    "True Positive (TP) refers to a sample belonging to the positive class being classified correctly.                                                                                                                                       \n",
    "True Negative (TN) refers to a sample belonging to the negative class being classified correctly.                                                                       \n",
    "False Positive (FP) refers to a sample belonging to the negative class but being classified wrongly as belonging to the positive class.                                              \n",
    "False Negative (FN) refers to a sample belonging to the positive class but being classified wrongly as belonging to the negative class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95d344-29c4-418a-9b16-c21295b8384a",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c7aa6-d45f-4a50-863d-f73d332028d6",
   "metadata": {},
   "source": [
    "Classification Accuracy: It is one of the important parameters to determine the accuracy of the classification problems. It defines how often the model predicts the correct output. It can be calculated as the ratio of the number of correct predictions made by the classifier to all number of predictions made by the classifiers. The formula is given below:            \n",
    "                                      \n",
    "Accuracy = (TP + TN)/(TP+TN+FP+FN)                             \n",
    "\n",
    "Misclassification rate: It is also termed as Error rate, and it defines how often the model gives the wrong predictions. The value of error rate can be calculated as the number of incorrect predictions to all number of the predictions made by the classifier. The formula is given below:                                                                                \n",
    "\n",
    "Error rate = (FP + FN)/(TP+TN+FP+FN)                                                                                                         \n",
    "\n",
    "Precision: It can be defined as the number of correct outputs provided by the model or out of all positive classes that have predicted correctly by the model, how many of them were actually true. It can be calculated using the below formula:                                                                                                                                          \n",
    "precision = TP/(TP + TN)                                                                                                                                                                                                \n",
    "\n",
    "Recall: It is defined as the out of total positive classes, how our model predicted correctly. The recall must be as high as possible.                                                                                                                                                          \n",
    "\n",
    "Recall = TP/(TP+FP)                                                                                                                                                                                                                            \n",
    "\n",
    "F-measure: If two models have low precision and high recall or vice versa, it is difficult to compare these models. So, for this purpose, we can use F-score. This score helps us to evaluate the recall and precision at the same time. The F-score is maximum if the recall is equal to the precision. It can be calculated using the below formula:                                                                                                                                                                                                                             \n",
    "F-measure = 2*((Recall * Precision)/(Recall + Precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef31a8-9cc7-4858-8c4e-1a3c86fa07de",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771cebeb-88ae-49f0-9edf-961881f893f1",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN)/(TP+TN+FP+FN)                                                                               \n",
    "where TP is True Positive                                                                                                \n",
    "      TN is True Negative                                                                                                                                \n",
    "      FP is False Positive                                                                                                                                 \n",
    "      FN is False negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe433d73-7517-4be2-9892-726c46518c99",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c890fa-f0c5-44f4-a69e-aa67432fa645",
   "metadata": {},
   "source": [
    "A confusion matrix is a valuable tool for evaluating the performance of a machine learning model, and it can also help identify potential biases or limitations in your model, especially when dealing with classification tasks. Here's how you can use a confusion matrix for this purpose:                                                           \n",
    "\n",
    "Understand the Basics of a Confusion Matrix:                                                                   \n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model.\n",
    "It consists of four main components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "Review Class Imbalances:                                                          \n",
    "\n",
    "Check for class imbalances in your dataset. If one class significantly outnumbers the others, the model may be biased towards the majority class, and this can skew the results.\n",
    "Check for Disproportionate Errors:                                                                           \n",
    "\n",
    "Analyze the confusion matrix to see if the model is making significantly more errors (FP or FN) for certain classes. Biases may be present if the model consistently misclassifies certain groups more than others.                              \n",
    "Examine Sensitivity and Specificity:                                                                                                              \n",
    "\n",
    "Sensitivity (True Positive Rate) and Specificity (True Negative Rate) are crucial metrics for understanding bias.\n",
    "Sensitivity measures the model's ability to correctly classify positive instances, which can be important when dealing with underrepresented or sensitive groups.                                                        \n",
    "Specificity measures the model's ability to correctly classify negative instances.                                                              \n",
    "Calculate and Compare Error Rates:                                                                                                                                                       \n",
    "\n",
    "Compute the error rates for different classes or groups within your data. Biases can manifest as higher error rates for certain groups, indicating that the model is less accurate when dealing with those groups.                                            \n",
    "Analyze Misclassification Patterns:                                                                         \n",
    "                                                                      \n",
    "Review the specific cases where the model made errors. Look for patterns in the misclassifications, such as whether certain features or characteristics are associated with misclassifications.                                                                               \n",
    "Consider Ethical and Domain Knowledge:                                                                                         \n",
    "\n",
    "Understand the domain of your problem and the potential ethical concerns. Some features or attributes may be sensitive or protected, and bias may arise from historical or societal factors.                                                \n",
    "Resampling and Data Augmentation:                                                                           \n",
    "\n",
    "If bias is identified, consider strategies like resampling or data augmentation to balance the dataset and mitigate bias. Techniques like oversampling, undersampling, or generating synthetic data can be helpful.                                        \n",
    "Fairness and Bias Mitigation Algorithms:                                                    \n",
    "\n",
    "Explore fairness-aware machine learning techniques and bias mitigation algorithms. These approaches can be used to train models that are more equitable and less biased.                                                                \n",
    "Iterate and Improve:                                                                                       \n",
    "\n",
    "Address the identified biases or limitations by refining your model, data collection, or preprocessing techniques.\n",
    "Reevaluate your model using the confusion matrix and other fairness metrics to track improvements.                   \n",
    "Remember that addressing bias in machine learning models is an ongoing process. Regularly monitor and reevaluate your model's performance to ensure it remains fair and unbiased, especially when deploying it in real-world applications where ethical and societal implications are significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
